{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import talib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, MultiHeadAttention, LayerNormalization, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the dataset\n",
    "crypto_data = pd.read_csv(\"data/raw/crypto_data_btc_usdt.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the timestamp column to datetime\n",
    "crypto_data['timestamp'] = pd.to_datetime(crypto_data['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ATR Feature\n",
    "crypto_data['ATR'] = talib.ATR(crypto_data['high'].values, \n",
    "                               crypto_data['low'].values, \n",
    "                               crypto_data['close'].values, \n",
    "                               timeperiod=14)\n",
    "\n",
    "# Bollinger Bands\n",
    "crypto_data['upper_band'], crypto_data['middle_band'], crypto_data['lower_band'] = talib.BBANDS(crypto_data['close'], timeperiod=20)\n",
    "\n",
    "# Moving Averages\n",
    "crypto_data['MA_7'] = talib.SMA(crypto_data['close'], timeperiod=7)\n",
    "crypto_data['MA_50'] = talib.SMA(crypto_data['close'], timeperiod=50)\n",
    "\n",
    "# On-Balance Volume (OBV)\n",
    "crypto_data['OBV'] = talib.OBV(crypto_data['close'], crypto_data['volume'])\n",
    "\n",
    "# Cumulative Volume Delta (CVD)\n",
    "# Assuming positive volume indicates buying and negative volume indicates selling\n",
    "crypto_data['buy_volume'] = crypto_data['volume'].where(crypto_data['close'] > crypto_data['close'].shift(1), 0)\n",
    "crypto_data['sell_volume'] = crypto_data['volume'].where(crypto_data['close'] < crypto_data['close'].shift(1), 0)\n",
    "crypto_data['CVD'] = crypto_data['buy_volume'] - crypto_data['sell_volume']\n",
    "crypto_data['CVD'] = crypto_data['CVD'].cumsum()\n",
    "\n",
    "# Cleanup: Remove temporary columns used for CVD calculation\n",
    "crypto_data.drop(['buy_volume', 'sell_volume'], axis=1, inplace=True)\n",
    "\n",
    "# Number of lags to introduce\n",
    "num_lags = 3\n",
    "\n",
    "# Create lagged features for the 'close' column\n",
    "for lag in range(1, num_lags + 1):\n",
    "    crypto_data[f'close_lag_{lag}'] = crypto_data['close'].shift(lag)\n",
    "\n",
    "# Handle NaN values\n",
    "crypto_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "train_size = int(len(crypto_data) * 0.8)\n",
    "train_data = crypto_data[:train_size]\n",
    "test_data = crypto_data[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a scaler for the feature columns and another for the target column ('close')\n",
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scalers on the training data and transform both train and test data\n",
    "\n",
    "# Columns that are not feature columns\n",
    "non_feature_cols = ['timestamp']\n",
    "\n",
    "# Dynamically generate the feature columns list\n",
    "feature_cols = [col for col in crypto_data.columns if col not in non_feature_cols]\n",
    "train_data_scaled = feature_scaler.fit_transform(train_data[feature_cols])\n",
    "test_data_scaled = feature_scaler.transform(test_data[feature_cols])\n",
    "\n",
    "# Fit and transform the target scaler on the 'close' prices\n",
    "train_close_scaled = target_scaler.fit_transform(train_data['close'].values.reshape(-1, 1))\n",
    "test_close_scaled = target_scaler.transform(test_data['close'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, target, seq_length):\n",
    "    sequences = []\n",
    "    sequence_targets = []\n",
    "    \n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i + seq_length]\n",
    "        seq_target = target[i + seq_length]\n",
    "        sequences.append(seq)\n",
    "        sequence_targets.append(seq_target)\n",
    "    \n",
    "    return np.array(sequences), np.array(sequence_targets)\n",
    "\n",
    "seq_length = 30\n",
    "X_train, y_train = create_sequences(train_data_scaled, train_close_scaled, seq_length)\n",
    "X_test, y_test = create_sequences(test_data_scaled, test_close_scaled, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerHyperModel(HyperModel):\n",
    "\n",
    "    def __init__(self, seq_length, n_features):\n",
    "        self.seq_length = seq_length\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = build_transformer_model(\n",
    "            seq_length=self.seq_length,\n",
    "            n_features=self.n_features,\n",
    "            d_model=hp.Int('d_model', min_value=32, max_value=128, step=32),\n",
    "            num_heads=hp.Choice('num_heads', values=[2, 4, 8]),\n",
    "            ff_dim=hp.Int('ff_dim', min_value=64, max_value=256, step=32),\n",
    "            dropout_rate=hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n",
    "        )\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG', default=1e-3)),\n",
    "                      loss='mse')\n",
    "        return model\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = RandomSearch(\n",
    "    TransformerHyperModel(seq_length=seq_length, n_features=X_train.shape[2]),\n",
    "    objective='val_loss',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='random_search',\n",
    "    project_name='transformer'\n",
    ")\n",
    "\n",
    "# Search for the best model\n",
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Retrieve the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation & Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Evaluate the best model on test data\n",
    "mse = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Mean Squared Error on Test Data: {mse:.4f}\")\n",
    "\n",
    "# Predict on test data using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "y_pred_original = target_scaler.inverse_transform(y_pred)\n",
    "y_test_original = target_scaler.inverse_transform(y_test)\n",
    "\n",
    "# Create a Plotly figure for visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for actual and predicted values\n",
    "fig.add_trace(go.Scatter(x=list(range(len(y_test_original))),\n",
    "                         y=y_test_original.flatten(),\n",
    "                         mode='lines',\n",
    "                         name='Actual',\n",
    "                         line=dict(color='blue')))\n",
    "fig.add_trace(go.Scatter(x=list(range(len(y_pred_original))),\n",
    "                         y=y_pred_original.flatten(),\n",
    "                         mode='lines',\n",
    "                         name='Predicted',\n",
    "                         line=dict(color='red', dash='dot')))\n",
    "\n",
    "# Update layout for dark mode and set height\n",
    "fig.update_layout(template=\"plotly_dark\",\n",
    "                  title=\"Bitcoin Price Prediction using Transformers\",\n",
    "                  xaxis_title=\"Time\",\n",
    "                  yaxis_title=\"Price (in USDT)\",\n",
    "                  height=800)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

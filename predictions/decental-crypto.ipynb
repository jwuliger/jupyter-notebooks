{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the necessary libraries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ccxt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 60  # Use the last 60 data points to predict the next one\n",
    "SPLIT_RATIO = 0.8  # Use 80% of the data for training and 20% for testing\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ohlcv(symbol, timeframe='1h'):\n",
    "    # Initialize the exchange\n",
    "    exchange = ccxt.binance()\n",
    "    \n",
    "    # Calculate the date from 2 years ago\n",
    "    two_years_ago = (datetime.datetime.utcnow() - datetime.timedelta(days=2*365)).isoformat()\n",
    "    \n",
    "    # Generate a Windows-compatible filename\n",
    "    file_name = f\"raw-{symbol.replace('/', '-').lower()}-data.csv\"\n",
    "    \n",
    "    # Check if the dataset already exists\n",
    "    if os.path.exists(file_name):\n",
    "        df = pd.read_csv(file_name)\n",
    "        since = exchange.parse8601(df.iloc[-1]['timestamp'])  # Get the last timestamp\n",
    "    else:\n",
    "        since = exchange.parse8601(two_years_ago)  # Start from 2 years ago\n",
    "        df = pd.DataFrame()\n",
    "    \n",
    "    # Fetch OHLCV data\n",
    "    new_data = pd.DataFrame()\n",
    "    while True:\n",
    "        try:\n",
    "            ohlcv = exchange.fetch_ohlcv(symbol, timeframe, since)\n",
    "            if len(ohlcv) == 0:\n",
    "                break\n",
    "            since = ohlcv[-1][0] + 1  # Update the `since` variable to avoid fetching the same data\n",
    "            # Correct the column names\n",
    "            new_data = pd.concat([new_data, pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data: {e}\")\n",
    "            break\n",
    "    \n",
    "    # Remove overlapping data points from both dataframes\n",
    "    if 'timestamp' in df.columns and 'timestamp' in new_data.columns:\n",
    "        overlap_timestamp = new_data['timestamp'].min()\n",
    "        df = df[df['timestamp'] < overlap_timestamp]\n",
    "        new_data = new_data[new_data['timestamp'] >= overlap_timestamp]\n",
    "    \n",
    "    # Reset indices and append the new data\n",
    "    df = df.reset_index(drop=True)\n",
    "    new_data = new_data.reset_index(drop=True)\n",
    "    df = pd.concat([df, new_data], ignore_index=True)\n",
    "    \n",
    "    df.to_csv(file_name, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fetching the Data for 'BTC/USDT'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching 2 years of 1-hour data for 'BTC/USDT' using the fixed function\n",
    "data_fixed = fetch_ohlcv('BTC/USDT')\n",
    "data_fixed.head()  # Display the first few rows of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'close' prices for normalization\n",
    "close_prices = data_fixed['close'].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "normalized_data = scaler.fit_transform(close_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Sequences of Data for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:(i + seq_length)]\n",
    "        y = data[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "SEQ_LENGTH = 60\n",
    "X, y = create_sequences(normalized_data, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(X) * 0.8)\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_test, y_test = X[train_size:], y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM Model Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# First LSTM layer\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Second LSTM layer\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Third LSTM layer\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
